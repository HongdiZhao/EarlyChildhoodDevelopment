{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDI Capstone Project\n",
    "Name: Hongdi Zhao\n",
    "\n",
    "### Early Childhood Development Outcome prediction using Machine Learning\n",
    "- Dataset: UNICEF Multiple Indicator Cluster Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve,average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names  \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.col_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the cleaned dataset, and check some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata('MICS6_5_ch_clean.dta')\n",
    "df = df.drop(columns = ['heigh_for_age_WHO','weight_for_age_WHO','BMI_zscore_WHO'])\n",
    "df = df.dropna()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['outcome_identifyletter'].value_counts()) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the machine learning model using Logistics Regression\n",
    "\n",
    "and pickle the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata('MICS6_5_ch_clean.dta')\n",
    "df = df.drop(columns = ['heigh_for_age_WHO','weight_for_age_WHO','BMI_zscore_WHO',\n",
    "                       'outcome','outcome_read4words','outcome_recog_symbol',\n",
    "                            'outcome_pickupthing_2finger','outcome_dothing_independent',\n",
    "                            'outcome_getawother','year','has_hometoy','area','melevel'])\n",
    "df = df.dropna()\n",
    "# here is the logistics regression\n",
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "\n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.col_names].values\n",
    "    \n",
    "y = df['outcome_identifyletter'].values\n",
    "cat_f = ['country','mom_edu','windex5']\n",
    "num_f = ['earlyc_edu','age','has_shoptoy','num_books']\n",
    "\n",
    "X_prep = df.drop(columns = ['outcome_identifyletter','sex'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prep, y, test_size=0.35, random_state=42)\n",
    "\n",
    "CST = ColumnSelectTransformer(num_f)\n",
    "num_pipeline = Pipeline([('attribs_adder', CST),\n",
    "        ('std_scaler', StandardScaler())])\n",
    "\n",
    "full_pipeline = ColumnTransformer([(\"num\", num_pipeline, num_f),\n",
    "        (\"cat_2\", OneHotEncoder(sparse=False), cat_f)])\n",
    "\n",
    "pipe = Pipeline([(\"Column Transform\", full_pipeline),\n",
    "                 (\"Logistics Regression\", LogisticRegression())])\n",
    "\n",
    "pipe.fit(X_train,y_train)\n",
    "print(\"Training Accuracy\", pipe.score(X_test,y_test))\n",
    "print((df['outcome_identifyletter'].value_counts()) / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pipe, open('ECD_logistics_identifyletters','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = full_pipeline.named_transformers_['cat_2'].categories_\n",
    "cat_f1_1 = []\n",
    "for i in a:\n",
    "    for j in i:\n",
    "        cat_f1_1.append(j)\n",
    "\n",
    "#coef = rf.feature_importances_\n",
    "coef = pipe.named_steps['Logistics Regression'].coef_\n",
    "feat = num_f + cat_f1_1\n",
    "\n",
    "for feature, coeffcient in (sorted(zip(coef[0,:],feat), reverse=True)):\n",
    "    print(feature, coeffcient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model\n",
    "- Confusion matrix\n",
    "- Cross validation score\n",
    "- Precision and recall curve\n",
    "- ROC curve\n",
    "- Calibration graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the above model Confusion Matrix with sklearn\n",
    "confusion_matrix(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation score\n",
    "# y_train_pred = cross_val_score(pipe, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
    "# y_train_pred\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_scores = cross_val_predict(pipe, X_train, y_train, cv=3,\n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "\n",
    "plt.savefig(\"precision_vs_recall_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    \n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) \n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)   \n",
    "    plt.grid(True)                                            \n",
    "\n",
    "plt.figure(figsize=(8, 6))                         \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.savefig(\"roc_curve_plot.png\")                        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train, pipe.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "y = df['outcome_identifyletter'].values\n",
    "cat_f = ['country']\n",
    "num_f = ['earlyc_edu','age','melevel','windex5',\n",
    "         'has_shoptoy','num_books']\n",
    "\n",
    "X_prep = df.drop(columns = ['outcome_identifyletter','sex'])\n",
    "X = full_pipeline.fit_transform(X_prep)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35,\n",
    "                                                    random_state=42)\n",
    "\n",
    "def plot_calibration_curve(est, name, fig_index):\n",
    "    \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n",
    "    # Calibrated with isotonic calibration\n",
    "    isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')\n",
    "\n",
    "    # Calibrated with sigmoid calibration\n",
    "    sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')\n",
    "\n",
    "    # Logistic regression with no calibration as baseline\n",
    "    lr = LogisticRegression(C=1., solver='lbfgs')\n",
    "    \n",
    "    # Logistic regression with no calibration as baseline\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    fig = plt.figure(fig_index, figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    for clf, name in [(lr, 'Logistic'),\n",
    "                      (rf, 'Random Forest'),\n",
    "                      (est, name),\n",
    "                      (isotonic, name + ' + Isotonic'),\n",
    "                      (sigmoid, name + ' + Sigmoid')]:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "        else:  # use decision function\n",
    "            prob_pos = clf.decision_function(X_test)\n",
    "            prob_pos = \\\n",
    "                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "\n",
    "        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())\n",
    "        print(\"%s:\" % name)\n",
    "        print(\"\\tBrier: %1.3f\" % (clf_score))\n",
    "        print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\n",
    "        print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\n",
    "        print(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\n",
    "\n",
    "        fraction_of_positives, mean_predicted_value = \\\n",
    "            calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "\n",
    "        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "                 label=\"%s (%1.3f)\" % (name, clf_score))\n",
    "\n",
    "        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "                 histtype=\"step\", lw=2)\n",
    "\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "    ax2.set_xlabel(\"Mean predicted value\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot calibration curve for Gaussian Naive Bayes\n",
    "plot_calibration_curve(GaussianNB(), \"Naive Bayes\", 1)\n",
    "plt.savefig('calibration_GaussianNB.png',dpi=199)\n",
    "\n",
    "\n",
    "# Plot calibration curve for Linear SVC\n",
    "plot_calibration_curve(LinearSVC(max_iter=10000), \"SVC\", 2)\n",
    "plt.savefig('calibration_LinearSVC.png',dpi=199)\n",
    "\n",
    "# Plot calibration curve for Random Forest SVC\n",
    "plot_calibration_curve(RandomForestClassifier(),\"RF\", 3)\n",
    "plt.savefig('calibration_RandomForest.png',dpi=199)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above model is everything for logistics regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other models:\n",
    "#### I also tried other models to see what would be the prediction accuracy\n",
    "You can find the code as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata('MICS6_5_ch_clean.dta')\n",
    "df = df.drop(columns = ['heigh_for_age_WHO','weight_for_age_WHO','BMI_zscore_WHO'])\n",
    "df = df.dropna()\n",
    "\n",
    "y = df['outcome_identifyletter'].values\n",
    "cat_f = ['area','sex','country']\n",
    "num_f = ['earlyc_edu','age','melevel','windex5','has_hometoy',\n",
    "         'has_shoptoy','year','num_books']\n",
    "\n",
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.col_names].values\n",
    "        \n",
    "CST = ColumnSelectTransformer(num_f)\n",
    "num_pipeline = Pipeline([('attribs_adder', CST),\n",
    "        ('std_scaler', StandardScaler())])\n",
    "\n",
    "full_pipeline = ColumnTransformer([(\"num\", num_pipeline, num_f), \n",
    "                                   (\"cat_2\", OneHotEncoder(sparse=False), cat_f)])\n",
    "\n",
    "X_prep = df.drop(columns = ['outcome','outcome_identifyletter',\n",
    "                            'outcome_read4words','outcome_recog_symbol',\n",
    "                            'outcome_pickupthing_2finger','outcome_dothing_independent',\n",
    "                            'outcome_getawother'])\n",
    "\n",
    "\n",
    "X = full_pipeline.fit_transform(X_prep)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=42)  # K-Folds cross-validator\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "#models.append(('SVM', SVC(kernel='rbf')))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train,y_train)\n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    msg = \"%s: %f %f (%f)\" % (name, cv_results.mean(),acc_score, cv_results.std())\n",
    "    print(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
